# Human-LLM Collaborative Annotation for Systematic Reviews
Welcome to the code repository for the paper "A Human-LLM Collaborative Annotation Method to Enhance Efficiency and Reliability in Article Screening for Systematic Reviews". This repository contains the code and resources needed to replicate the experiments and methods described in the paper.

# Overview
Systematic reviews are crucial for evidence-based medicine, but the manual annotation of articles is often labor-intensive and time-consuming. In this study, we introduce a novel annotation method that combines the efficiency of large language models (LLMs) like GPT-3.5 with human expertise. This method significantly reduces the annotation workload while maintaining high reliability.

# Key Features:
- **Prompt Optimization**:  Scripts for refining LLM prompts using a standard dataset to achieve near-perfect recall.
- **Collaborative Annotation Process**:  Code for integrating LLM annotations with human verification.
- **Performance Evaluation**: Tools for evaluating the F1 score and other metrics, comparing the collaborative method to traditional manual annotation.

